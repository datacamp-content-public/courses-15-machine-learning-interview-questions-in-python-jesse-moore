---
title: Insert title here
key: 043ea510d97468d9de9662eddff29a67

---
## Question 7: Handling and Preparing Time Series Data for Training

```yaml
type: "TitleSlide"
key: "c3b9a3d56a"
```

`@lower_third`

name: Jesse Moore
title: Data Scientist


`@script`
In this video, you'll learn how to answer interview questions on preparing time-series data for machine learning.


---
## What is Time Series Data?

```yaml
type: "FullSlide"
key: "93726b0c11"
center_content: false
```

`@part1`
![](https://i.imgur.com/lAm3EKQ.png){{1}}

- Time series data is time-ordered, typically in even spaces of time{{2}}

- Needs to be handled in a special way when preparing training and validation data{{3}}


`@script`
Time series data is a special type of data.

One of the most obvious examples of time-series data is the stock market. The prices in the previous days are very predictive of the prices today.

Due to this chronological nature, we need to be very cognizant of the potential pitfalls and understand how to deal with them.


---
## Preparing Time Series Data

```yaml
type: "FullSlide"
key: "d40ee49e17"
```

`@part1`
1. Creating features for a predefined number of timesteps{{1}}

2. Creating a training and validation set{{2}}


`@script`
In preparing time series data, we have two main steps.

1. Creating features using a predefined number of time steps

2. Creating a training and validation set taking into account the unique handling required for time series data.

Let's get started.


---
## Creating Features for Time-Series Models

```yaml
type: "TwoColumns"
key: "d0916f861d"
disable_transition: false
```

`@part1`
Madrid Air Quality Data {{1}}

```python
air_df.head()
```{{2}}


`@part2`
![](https://i.imgur.com/31eQGJ8.png){{2}}


`@script`
As mentioned, we need to create features for the time series model.

For our example, we are going to look at a simplified version of the Madrid Air Quality dataset from Kaggle. 

We have available air_df, a dataset that contains data for one sensor in Madrid for all of 2017. 

Air_df contains one row for each hour in the day. The two features we will work with are PM10 and PM2.5. 

These are the sensor readings for particles under 10 micrometers and 2.5 micrometers respectively. 

All measurements are in micrograms per cubic meter.


---
## Using the Shift Function

```yaml
type: "FullCodeSlide"
key: "cdb65d4aab"
disable_transition: true
```

`@part1`
```python
air_df.sort_values('date', inplace=True)
```{{1}}

```python
air_df['PM10_t-1'] = air_df['PM10'].shift(1)
```{{2}}

![shifted dataframe](https://i.imgur.com/emxW0tB.png){{3}}


`@script`
To build our features, we first need to make sure that all data is sorted according to the timestamps. 

Then we will make use of the Pandas "shift" function. 

The shift function works by shifting a column down by a number of slots. By providing a higher number to the shift function, we can append data from several previous time steps.

Here we have appended one previous time step to the data.


---
## Using the Shift Function

```yaml
type: "FullCodeSlide"
key: "6f9c9d4ab2"
```

`@part1`
```python
air_df.sort_values('date', inplace=True)
```

```python
air_df['PM10_t-1'] = air_df['PM10'].shift(1)
```

![shifted dataframe](https://i.imgur.com/sQsFwrQ.png)


`@script`
We can see that the PM10 values from the first row have been shifted one row and appended to the second row. This occurs for every row in the dataset.


---
## Preparing the Air Quality Data

```yaml
type: "FullCodeSlide"
key: "6b0ce02ffb"
```

`@part1`
```python
air_df.sort_values('date', inplace=True)
```{{1}}

```python
for timestep in range(1, 4, 1): #3 timesteps
    air_df['PM10_t-' + str(timestep)] = air_df['PM10'].shift(timestep)
    air_df['PM25_t-' + str(timestep)] = air_df['PM25'].shift(timestep)
```{{2}}

```python
air_df.dropna(inplace=True)
air_df.drop('PM10', axis=1, inplace=True)
```{{3}}

```python
air_df.head()
```{{4}}

![](https://i.imgur.com/st4eSYm.png){{4}}


`@script`
Now we're reading to prepare the actual data. 

I want to use data from three previous time steps to predict the PM 2.5 concentration from the next time step.

First, we sort the data.

Second we create a for loop that iterates over the number of time steps we want to use in our prediction and adds the properly identified rows. We do this for both PM10 and PM2.5 values

Finally, we drop the NA values produced from shifting the data as well as the "PM10" value for each time step as we will only be predicting PM2.5.

Our dataset is ready for splitting!


---
## Training and Validation Sets

```yaml
type: "FullCodeSlide"
key: "f3c3126a1a"
```

`@part1`
The wrong way!!!{{5}}

```python
train_df = air_df.sample(frac=0.75)
test_df = air_df[~air_df.index.isin(train_df.index)]
```{{1}}
- Random sampling assumes there is no relationship between data points{{2}}

- Be careful to avoid "Look-ahead" bias{{3}}

- Keep in mind what would be available in a live model{{4}}


`@script`
If we weren't working with time series data, we would likely split the data using a version of random sampling. One common way is to use Pandas sample function, which randomly samples a certain fraction of the data.

What might be wrong with this method?

Remember how we used Pandas shift function. Since we randomly sampled the data, the algorithm will certainly have access to future information when making predictions.

In this case, your algorithm could determine from some of the features in the dataset to "Look Ahead".

As a result, your algorithm would test very well but perform terribly in practice.

Let's take a look at a better method.


---
## Training and Validation Sets

```yaml
type: "FullImageSlide"
key: "8863dc96bd"
```

`@part1`
![](https://i.imgur.com/APiGy52.png)


`@script`
When preparing the data, we need to ensuring that the training data precedes the validation data. 

In the chart, the red area denotes the data that will be set for training and the blue area for validation. 

Using this method we can avoid the most common pitfalls when modelling time series data.


---
## Training and Validation Split (The Right Way)

```yaml
type: "FullSlide"
key: "53a1620a33"
```

`@part1`
```python
num_rows = air_df.shape[0]
```{{1}}
```python
split_ix = round(num_rows * 0.75)
```{{2}}
```python
train_df = air_df[0:split_ix]
val_df = air_df[split_ix:]
```{{3}}

```python
print(train_df.date.max() < val_df.date.min())
#True
```{{4}}

```python
val_df.drop('date', axis=1, inplace=True)
train_df.drop('date', axis=1, inplace=True)
```{{5}}


`@script`
Our data is already ordered and ready to go, so we need to find the index that covers the training part of our data. We will use the first 75% of our dataset.

First we get the number of rows from the shape attribute. 

Second, we use the round function to get the integer index that will mark the right training/test split of the dataset.

Third, we create our training and validation dataframes using the split we have just found.

Fourth, we check that the maximum value in the training set is less than the minimum value in the validation set.

And, finally, we drop the 'date' column from our prepared data before making predictions.


---
## Try it for yourself

```yaml
type: "FinalSlide"
key: "7c729ed8fa"
```

`@script`
Time for you to try it out yourself.

